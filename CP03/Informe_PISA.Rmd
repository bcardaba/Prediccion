---
title: "Informe PISA - Splines - GAM"
author: "Beatriz Cárdaba"
date: "09/11/2020"
output: html_document
---

__El objetivo es modelizar la relación entre la puntuación media (OSS) y el resto de variables, utilizando modelos de splines y GAM. Se debe realizar CV cuando se pueda.__


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)
```

```{r}
library(tidyverse)
library(broom) # modelos en df
library(flextable) # Tablas formateadas
library(mgcv) # estimar gam
library(reshape2) # melt
library(readr)
library(dplyr)
library(skimr)
library(here) # Comentar
library(janitor) # Clean names
library(magrittr) # Pipe operators
library(corrplot) # Correlations
library(ggcorrplot)  # Correlations
library(PerformanceAnalytics) # Correlations
library(leaps) # Model selection
library(glmnet)
library(gam)
library(fBasics)
library(nortest)
library(MASS)
library(imputeTS)
library(gam)
```


# Import Data
```{r}
raw_data <- read_csv("pisasci2006.csv")
```

# EDA
Realizamos un análisis exploratorio de los datos para conocer sus cararterísticas.

En primer lugar conocemos las variables y los tipos de datos de contienen:

```{r ,echo = FALSE}
str(raw_data) # visualizamos el tipo de variables de la base de datos, el número de observaciones y de variables
```


```{r }
raw_data %<>% clean_names() # %<>% es un doble pipe, mete row_Data en clean_nnba y el resultado lo guarda en rowdata
# raw_data <- clean_nnba(raw_data)
# raw_data <- raw_data %>% clean_nnba()
# lo que hace es que nos limpia los nombre, todas en minusculas quitar los puntos
colnames(raw_data) 
```

## Diccionario de Variables

 - country: país examinado en el informe
 - overall: Overall Science Score (average score for 15 year olds)
 - issues: 
 - explain:
 - evidence:
 - Interest:Interest in science
 - Support: Support for scientific inquiry
 - income: Income Index
 - health: Health Index
 - edu: Education Index
 - hdi: Human Development Index (composed of the Income index, Health Index, and Education Index)
 
 
## Resumen de datos

visualñizamos las caraterísticas de cada dato, en especial las medidas estadísitcas, como la media la desviación típica o los cuartiles.

```{r,echo = FALSE}
dim(raw_data)
skim(raw_data)
```

## Data Wrangling data

 Data Wrangling es el proceso de limpiar y unificar conjuntos de datos complejos para su análisis, lo que facilita el us de los datos.
 De los datos visulizados en el resumen posemos observar que hay datos na, procedemos a eliminarlos para que no nos supongan un porblema:

Visualizamos  NA y los duplicados:

COmo hemos visto, el data set cuenta con 68 obsservaciones. En el resumen anterior podemos conocer los valores NA por columna "n_missing", observamos que son bastantes valores respecto del total de la muetsra, en algunos casos super al 10%, por eso en este caso en vez de eliminar los datos NA, sustituiremos por la media de la variable:

```{r}
raw_data <- na_mean(raw_data) # sustituimos los NA por la media de la variable
# Remove duplicate rows of the dataframe
raw_data %<>% distinct(country,.keep_all= TRUE) # quitamos los duplicados en base a los nombres de los países
```

```{r}
data <- raw_data# renombramos los datos, ya que hemos realizado todas las transformaciones
# Remove duplicate rows of the dataframe

skim(data)
```


Observamos que hay una variable categórica, que es el país y 10 variables numéricas. 
Finalmente tras eliminar los NA vamos a trabaja con un overall de 52 observaciones, correspondientes a 52 países diferentes.

## EDA

_Gráficos de dispersión:_

Representamos graficame las diferentes relaciones entre OSS (overall) y el resto de variables que indica el enunciado que son las imoortantes a tener en cuenta en este modelo:
```{r}

# Dibijamos los gg plot para saber la forma de la distribución de los datos, sobre todo si son o no liemales
par(mfrow=c(2,3))

ggplot(data = data, mapping = aes(x = overall, y = interest)) + 
     geom_point() +
  geom_smooth()

ggplot(data = data, mapping = aes(x = overall, y = support)) + 
  geom_point() +
  geom_smooth()

ggplot(data = data, mapping = aes(x = overall, y = income)) + 
  geom_point() +
  geom_smooth()

ggplot(data = data, mapping = aes(x = overall, y = health)) +
  geom_point() +
  geom_smooth()

ggplot(data = data, mapping = aes(x = overall, y = edu)) +
  geom_point() +
  geom_smooth()

ggplot(data = data, mapping = aes(x = overall, y = hdi)) +
  geom_point() +
  geom_smooth()
```
En estos gráficos podemos apreciar que las correlaciones entre las variables independientes y la variable dependiente overall no son lineales.

Vamos a comprobar como sería si se estableciera un modelo lineal:


```{r}
# Fit a linear model
par(mfrow=c(2,3))
lm_mod <- lm(overall~ interest + support + income + health + edu + hdi, data = data)

width(flextable(tidy(lm_mod)), width = 2)

width(flextable(glance(lm_mod)), width = 2)
      
# Visualize the model
termplot(lm_mod, partial.resid = TRUE, se = TRUE)
```

En estas gráficas nos reafirmamos en que existen relacione no lienales entre las variables del estudio.

Vamos a estudirar si esxiten correlaciones entre las variables:

_Correlación entre variables_
```{r}
# Degres of the polynomial#
###########################
ggcorrplot(cor(data %>% 
               select_at(vars(-country,-evidence,-explain)), 
            use = "complete.obs"),
            hc.order = TRUE,
            type = "lower",  lab = TRUE)
```


Apreciamos que existen correlaciones tanto positivas como negativas.

```{r}
chart.Correlation(data %>%  #las lineas rectas rojas, indican que no hay correlacion
               select_at(vars(-country,-evidence,-explain)),
               histogram=TRUE, pch=19)
```
Mediante las gráficos de correlación comprobamos que las variables están correlacionas entre sí.


## Splines:

Los splines de regresión es una técnica de regresión no paramétrica. Esta técnica de regresión divide cada variable del conjunto de datos en intervalos separados por puntos llamados nudos y cada parte tiene su ajuste separado.

En los smooth splines no se tiene que elegir el número y posición de los knots, ya que hay uno en cada observación. En su lugar, se tiene que escoger un valor de λ que determine como de estricta es la penalización. 
Si λ =  0 el término de penalización no tiene ningún efecto, y la función estará turbia e interpolará cada valor.
Si λ tiende a infinito se da el mayor suvizado posible, una línea recta.

Una forma de encontrar el λ óptimo es mediante cross-validation. Lambda controla la rugosidad de la spline suavizado, y por lo tanto los grados efectivos de libertad.

### Grados de Libertad por Cross Validation:

Calculamos los grados de libertad para cada variables para conocer el λ de cada variable:


```{r}
dfinterest<-smooth.spline(x = data$interest,  y = data$overall, cv=TRUE)
dfsupport<-smooth.spline(data$support, data$overall, cv=TRUE)
dfincome<-smooth.spline(data$income, data$overall, cv=TRUE)
dfhealth<-smooth.spline(data$health, data$overall, cv=TRUE)
dfedu<-smooth.spline(data$edu, data$overall, cv=TRUE)
dfhdi<-smooth.spline(data$hdi, data$overall, cv=TRUE)
``` 
```{r}
dfinterest$df
dfsupport$df
dfincome$df
dfhealth$df
dfedu$df
dfhdi$df
``` 

## GAM

El modelo GAM es un modelo Aditivo por lo que podemos intrucir todas las vairables e irlas sumando con su correspondiente grado de libertad que hemos calculado mediante cross validation

En este caso todas las variables que vamos a analizar son continuas, por lo que en totdas se puede introducir el parametro "s" para que se smooth

```{r}
gam<- gam(overall ~ s(interest, 4.750171) + s(support, 2.001243) + s(income, 4.244952) + s(health, 2.002844) + s(edu, 2.002385) + s(hdi, 8.603228), data = data)
summary(gam)
```

Representamos graficamente la salidad para la regresión GAM:


```{r}
library(visreg)
par(mfrow=c(2,3))
visreg(gam)
``` 

 De la salida observamos que no son significativas las variables health y education en los efectos parametricos por lo que quitamos el efecto smooth

```{r}
gam_2<- gam(overall ~ s(interest, 4.750171) + s(support, 2.001243) + s(income, 4.244952) + (health) + (edu) + s(hdi, 8.603228), data = data)
summary(gam_2)
```
Representamos graficamente el modelo GAM 2


```{r}
library(visreg)
par(mfrow=c(2,3))
visreg(gam_2)
```   
De la salida podemos observar que tanto el AIC como el error disminuyen levemente en GAM 2.

